---
title: 毕业答辩讲稿
date: 2020-06-10T09:54:47+08:00
lastmod: 2020-06-10T09:54:47+08:00
author: Author Name
cover: https://tva1.sinaimg.cn/large/007S8ZIlly1gfncjjxngqj30rs0ijq3s.jpg
categories: ["论文答辩"]
tags: ["毕业", "答辩"]
# showcase: true
---

难得一遇线上答辩，怎能放过预备讲稿。

<!--more-->

## 开篇介绍

各位老师们好！我是邓一帆，我的毕设题目是《基于神经网络的篇章级关系抽取技术研究》,下面将主要分为四部分内容来讲一下我的工作，分别是我的研究思路与方法，对研究结果的分析，研究结果的实际应用意义以及最后的工作总结

## 研究思路与方法

那么首先介绍一下我的研究思路，我在做联合抽取时选用的是共享参数的方式，而没有选用标注策略，为什么使用共享参数可以先看一下下面这张图，图中展示了实现这两种思路的论文模型特点，需要说明的是，这两种模型的实验数据集都是句子级别的，所以关系标签数也会少一点，其中基于标注策略的方式需要根据现有的标签生成新的标注策略，新的标签数=4 * ｜原标签数｜ * 2 + 1，Zheng等人所用数据集共有24种标签，因此生成的新标签数也不算特别多，但是我选用的DocRED数据集共有97种标签，这里计算了一下新标签数，竟然达到了777种，这对于只有3000条训练数据的情况显然是不合适的，因此选用了共享参数的方式来做从句子级别到篇章级别的迁移探索

## 对原有模型的改进

我对原有的共享参数模型做了相应的改进使之能够适应长文本的处理，从表格里边可以看出，为了尽可能降低面对长文本的学习焦点丢失问题，我将原有的RC模块拆分成了两个模块，分别完成关系实体对的识别以及关系分类，将不平衡分类单独抽离出来降低对整个模型的影响，增加模型对不平衡数据的关注程度，实现了学习聚焦。同时在结果合并时加入了阀值影响，提高结果的精准率。在NER模块增加了CRF层学习标签间的依赖。通过这些工作增强了原有模型对长文本的处理能力。

## 处理流程

下边将详细介绍下数据在模型中的流动形式以了解模型的处理过程。

首先是预处理工程。由于数据集中的数据都是以类似左边这张图的形式来存储的，不符合深度学习的输入数据规范，因此需要先将文本中的单词转为词向量，针对每个单词的字符转为单词的字符级向量，这里的向量代表的都是其对应的索引。然后需要将关系类型也转为索引。

之后会通过DataSet和DataLoader生成批次数据，在这个过程中，会构建好特征工程，也就是实体对的位置和距离特征，将文本对应的文本向量送入共享参数层进行编码，将输出的编码参数分别输入到三个子任务模块，右边这张图展示的是数据在NER模块流动的情形。可以看到，在这一模块会先融合单词的字符特征来作为输入，LSTM的输出结果给出了单词属于各个类别的概率，为了学习到标签之间的依赖，比如B标签后边不能是S标签等等，将输出送入CRF层处理，计算出标签的最优路径，得出黄色高亮的这一组最符合逻辑，那么就会成为最终输出。

这一模块处理完后，会将输出结果作为后边的实体类型特征分别输入到RC模块和RC_NA模块，先从RC模块看起，将共享参数与实体类型特征融合作为输入，输出结果再与实体的位置特征相乘并与实体对的距离特征进行特征拼接，得到的特征向量送入BiLiner进行处理，结果中p代表实体对，也就是第一对实体对属于Country类型的概率最高，达到了98%。

接下来看RC_NA模块，这一模块主要负责识别有关系的实体对，处理过程与关系分类的过程类似，只不过输出的结果是一个二分类结果，代表了实体对有关系的概率，比如图中实体对1有关系的概率高达89%，这是非常高的。

接下来还有一个结果合并的过程，先将两个模块的结果使用argmax（）函数处理得到分类类型，然后进行逐元素相乘，这里会涉及到一个阀值的影响，比如p1有关系的概率是89%，那么阀值等于0时，p1就被认为是有关系的，阀值等于0.9时，p1则会被认为是无关系的，通过对阀值的控制减弱不平衡数据对分类的影响。

## 结果分析

可以看到，在训练了3k多个batch后，训练集的f1值已慢慢接近于平缓，但是此时通过实验发现测试集的f1值已经开始摇摆不定并且有下降趋势，呈现出典型的过拟合症状。并且当把阀值调为0时，recall值达到了78.67%，也就是关系分类的精准率和召回率都是不低的，但是当把bias调至0.75时，recall值却仅有41.06%，这个现象表明了RC_NA的不平衡二分类任务已经成为了模型的瓶颈，并且在试过一些方法后没有看到太大的成效。

## 研究成果的应用意义

关系抽取得到的结构化知识图谱可以在许多领域得到应用，典型的有搜索引擎的优化，就比如这张图所展示的，平常的url关键词匹配是不涉及到知识的，匹配的相似度高并不代表符合用户的需求，但是第一条使用了知识图谱的搜索结果就很好的满足了用户需求，只需要将与实体“吴亦凡”拥有“身高”关系的实体展示出来就行，这对于图数据库来说是非常快的。

还有搜索的相关推荐功能，也可以将与实体有相同关系的实体集合展示出来，作为知名校友模块或者相关院校模块等等。

## 总结

下面是对我所做工作的一个总结，我所做工作的特点相比与原有的共享参数联合抽取来说，主要有下面四点：拆分了原来的关系抽取模块，增加了合并阀值，使用CRF增加标签依赖，使用了实体对的位置和距离特征

通过对实验结果的分析来看，未来的工作可以从下面四个方面来考虑：丰富特征，大小写往往对区分实体很有用，人名、机构名往往都是大写开头；增加样本数，目前样本数太少，导致过拟合比较严重；优化损失函数，比如使用Focalloss解决不平衡二分类任务，由于谷歌服务器的配置有限，本次实验无法使用这一损失函数；还可以探索使用其他的神经网络模型。

## 结语

我的汇报到此结束，非常感谢各位老师的指导与帮助！
